# HDFS小文件合并

## 小文件过多的危害

已知HDFS中默认block的大小为128MB，HDFS上存储了许多远远小于128MB的小文件，这些小文件每个都占用一个128MB的block，当小文件数量过多时，十分浪费硬盘空间。

另一方面，Hadoop可以很好的处理大文件，Hadoop处理文件时会把每一个文件传递给map函数，Hadoop调用map函数时会创建一个映射器，当文件过多时会创建大量映射器，使程序执行效率降低。



## 解决方案

对小文件进行合并。

在客户端上传文件时执行一定的策略，对小文件进行预先合并，或者使用Hadoop的`CombineFileInputFormat<K,V>`实现小文件合并。

